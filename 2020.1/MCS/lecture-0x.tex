\lecture{X}{19.06.2020}{Optimal Control}

\section*{Review}

Review of what's been seen so far.

Our system is
\[
    \dot{x} = Ax + Bu
\] \[
y = Cx + Du
\] 

The critical values (equivalent to roots of the transfer function) are the eigenvalues of the characteristic equation, that is \[
    det(\lambda I-A) = 0
\] .

Stability means that \[
    Re(\lambda_i)<0 \forall i
\] .

Control design.

Pole placement approach: if we have order $n$, define $n$ eigenvalues for the closed loop $P_1, \ldots, P_n$. Steps:
\begin{enumerate}
    \item Controlability check \[
    W_c = \left[ B | AB | \ldots | A^{n-1}B \right] 
\] $rank(W_c) = n \implies $ controlability
    \item Desired polynomial \[
	    P_d = \prod_{j=1}^{n} (\lambda-P_j) 
    \] . If the system is controllable we can achieve this polynomial as the characteristic function, that is, we define $B k$ such that \[
    P_{closed loop} = det(\lambda I - A + B k)
    \] \[
    = P_d
    \] .
\end{enumerate}

If we can't measure our state variables, we can design an \textbf{observer}. For that we need to check the observability of the system, that is, given \[
    W_o = \begin{bmatrix} C \\ CA \\ \ldots \\ CA^{n-1}\end{bmatrix} 
\] we need \[
rank(W_o) = n
\] . If that is verified, we determine the poles we want for the observer as \[
P_o = \prod_{i=1}^{n} (\lambda - P_i) 
\] and so we define L such that \[
P_{ob} = det(\lambda I - A + LC)
\] \[
P_{ob} = P_o
\] .

This way, our observer is defined as \[
    \hat{x} = A \hat{x} + Bu + L(y - \hat{y})
\] where \[
\hat{y} = C \hat{x} + Du
\] with $D=0$ for simplicity. In this scenario, our state feedback gain $k$ is applied to $\hat{x}$.


\section*{Optimal Control (LQR)}

Optimal control is the task of designing a controlling by looking beyond performance, so optimizing also the \emph{costs} associated.

The idea is that we want to define the control input  $\bm{u}(t)$ in an interval $t_0 \le t\le t_f$ such that the state of the system goes from an initial value to a desired final value minimizing a cost function. So we formulate this as an optimization problem with the initial and final states as the restrictions (more restrictions may exist). Note that the values that $\bm{x}(t)$ assumes during the time interval define a \emph{trajectory}.

Regarding the solutions to this problem:

\begin{enumerate}
    \item Certain combinations of cost functions and dynamic system models have analytic solutions;
    \item In a wide class of problems, we can find control laws;
    \item The speed of modern computers can perform the calculations in real time, which allows us to recompute the trajectories.
\end{enumerate}

\subsection*{The Optimal Control Problem}

The general optimization problem can be seen as the minimization of the cost function \[
J=\phi(\bm{x}(t_f),t_f) + \int_{t_o}^{t_f} \L(\bm{x}(t),\bm{u}(t),t)dt 
\] where $\phi$ is the \emph{final state penalty} and the integral of $\L$ is the cost associated with the states  trajectory. Note that $\phi$ depends on both the states on the final time and the final time $t_f$ itself, so we can formulate both the minimization of the final state position ($\bm{x}(t_f)$) and the minimization of the final time.

Also, note how the equation is limited to $t_f$, that is, we do not look after this moment, which means that the classical idea of stability (BIBO) does not make sense, as we are not looking to what happens after a certain point and, therefore, both the input and output are always bounded.

Back to the problem formulation, we also add a constraint \[
    \bm{c}(\bm{x}) = 0
\] which implies that the solution has to remain on the surface defined by $\bm{c}$.

We know that the states are dictated by the dynamics of the (linear) system. So the following must be satisfied \[
    \dot{\bm{x}} = A\bm{x} + B\bm{u}
\], that is, it is a constraint to the optimization if we formulate it as \[
    \dot{\bm{x}} - A\bm{x} - B\bm{u} = 0
\].

Therefore, our optimization problem can be stated as \[
    \min_{\bm{u}, t_f} J(\bm{x},\bm{u},t_f,t_0)
\] \[
\textrm{s.t. } \bm{c}(\bm{x}) = 0
\]. Of course this is subject to the problem and more constraints (other than that of the system model) can be added or the current can be loosened. \emph{E.g.,} the final state can be seen as a constraint.

So, then, to solve our optimization problem, we must find a solution to the derivatives of $J$ in the directions along the surface. That is, the gradients \[
    \frac{\partial J(\bm{x}^*)}{\partial x} + p \frac{\partial \bm{c}(\bm{x}^*)}{\partial x} = 0
\] for a solution $\bm{x}^*$. Which means we can now formulate a new unconstrained problem \[
    J_a(\bm{x}, p) = J(\bm{x}) + pc(\bm{x})
\] in which $p$ is the coefficient that defines the linear combination.

\subsection*{Optimization}

\begin{description}
    \item[Open-loop Parametric Optimization] when $\bm{u}(t)$ is assumed to be a known function defined by parameters $\bm{k}$, therefore we can write $\bm{u}(\bm{k},t)$.
\end{description}

In this case, the optimization problem can be seen as an optimization over $\bm{k}$. Therefore, we need to find \emph{local minima} of $J$, that is, we must find $\bm{k}$ such that \[
    \frac{\partial J}{\partial \bm{k}}=\bm{0}
\] and \[
\frac{\partial^{2}J}{\partial \bm{k}^{2}}>\bm{0}
\]. Note that this equations can be seen as a system of equations over the unknown variables $\bm{k}$, that is, in matrix form.

\begin{description}
    \item[Closed-loop Parametric Optimization] when  $\bm{u}(t)$ is a function over the dynamic state, that is, $\bm{u}(t) = \bm{\psi}[\bm{k},\bm{x}(t),t]$ where $\bm{\psi}[.]$ is the \emph{control law}.
\end{description}

\subsection*{Linear Quadratic Regulator (LQR)}

Beware: it applies \textbf{only to linear systems}.

Given a system defined as \[
    \dot{x}(t) = Ax(t) + B_u u(t)
\] \[
y(t) = C_y x(t)
\] in which $C_y$ can be the identity matrix, then we define our cost function as \[
J(x(t), u(t)) = \frac{1}{2}y^T(t_f)H_yy(t_f) + \frac{1}{2}\int_{0}^{t_f}\left[ y^{T}(t)Q_yy(t) + u^T(t)Ru(t) \right] dt
\] \[
x(0) = x_0
\]  in which $H_y$, $Q_y$, and $R$ are positive definite.

Note how the cost functions are made of quadratic functions. Also, it is important to note that in the integral, one is related to the states (since we defined or system without the direct influence of the input in the output), and the input.

Note how, in this formulation of the system, we are understanding our output as the same as our state variables, then $C_y$ can determine which are the states we are trying to control.

Remember that, for a quadratic function \[
    F(y) = y^TAy
\] in which $y$ is a vector and $A$ is a matrix, is the quadratic combination over the dimensions of $y$, so an example for the 2 dimensional case \[
\begin{bmatrix} y_{1} & y_2 \end{bmatrix} \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \begin{bmatrix} y_1\\ y_2 \end{bmatrix} = a_{11}y_1^{2}+(a_{12} + a_{21})y_1y_2 + a_{22}y_2^{2}
\] .

Normally, in the definition of our LQR terms, we set the non-diagonal terms of $Q_y$ are $0$, having only the main diagonal controlling the importance of each state.

Here, our problem is to bring our output to zero, which is a common approach to the regulator problem (since bringing it to any other value is a simple change).

\subsubsection*{Hamiltonioan Equation}

Integrating the constraints with the cost function, we have \[
    J_a(x(t),u(t),p(t)) = J(x(t),u(t)) + \int_0^{t_f}p^{T}(t)\left[ A(x(t) + B_uu(t) - \dot{x}(t) \right] dt
\] so substituting $y=C_yx$ and applying the transformation $H=C_y^{T}H_yC_y$ and $Q=C_y^{T}Q_yC_y$, we have \[
    J_a(x(t),u(t),p(t)) = \frac{1}{2}x^T(t_f)Hx(t_f) +
\] \[
\frac{1}{2}\int_{0}^{t_f}\left[ x^{T}(t)Qx(t) + u^T(t)Ru(t) + p^{T}(t)\left[ A(x(t) + B_uu(t) - \dot{x}(t) \right] \right] dt
\]. This way we enlarge our problem but transform it into an unconstrained problem.

To solve it we calculate the variation (?) of the function and achieve the following restrictions: 
\begin{equation*}
\begin{split}
    & p^{T}(t_f) = x^T(t_f)H \\
    & \dot{p}(t) = -x^T(t)Q - p^T(t)A \\
    & 0 = u^T(t)R + p^T(t)B_u \\
    & \dot{x}(t) = Ax(t) + B_uu(t)
\end{split}
\end{equation*}
From which we can derive \[
    u(t) = -R^{-1}B_u^Tp(t)
\] .

Now, removing $u(t)$, we simplify the problem to a set of differential equations
\begin{equation}
    \begin{bmatrix} \dot{x}(t) \\ \dot{p}(t) \end{bmatrix} = \begin{bmatrix} A & -B_uR^{-1}B^{T}_u \\ -Q & -A^T \end{bmatrix} \begin{bmatrix} x(t) \\ p(t) \end{bmatrix} 
\end{equation}
with initial conditions
\begin{equation*}
    \begin{split}
	& x(0) = x_0 \\
	& p(t_f) = Hx(t_f)
    \end{split}
\end{equation*}

From handling the general solution of this system of differential equations, we can achieve \[
    p(t) = P(t)x(t)
\] which implies in \[
u(t) = -R^{-1}B_u^{T}p(t) = -K(t)x(t)
\] so a time variant gain matrix. This way we can define our controller as a linear, time-varying state feedback. This way, we can summarize our approach to the problem to the calculation of $P(t)$.

\subsubsection*{Riccati Equation}

From $p(t)=P(t)x(t)$ we can derive the following relation \[
    \dot{p}(t)=\dot{P}(t)x(t) + P(t)\dot{x}(t)
\] which we can substitute in equation (1) together with the system equation for $\dot{x}(t)$. Through this, we achieve the \emph{Riccati Equation} \[
\dot{P}(t) + P(t)A + A^TP(t) + Q - P(t)B_uR^{-1}B_u^{T}P(t) = 0
\] together with the final condition \[
P(t_f) = H
\] . This has no feasible analytical solution, but the numerical solution (through backward integration) is very useful and doesn't need to be computed in real time.

\subsubsection*{Steady-State LQR}

If the controller is needed to remain active for a long period of time. This requires a formulation of the problem as \[
J(x(t), u(t)) = \frac{1}{2}\int_{0}^{\infty}\left[ x^{T}(t)Qx(t) + u^T(t)Ru(t) \right] dt
\] with the initial condition \[
x(0) = x_0
\] . We remove the first term because we know that the states are going to $0$.

Note that, by the definition, our system is stable, no matter the choice of $Q$ and $R$.

Now, our set of differential equations is given by
\begin{equation}
    \begin{bmatrix} \dot{x}(t) \\ \dot{p}(t) \end{bmatrix} = \begin{bmatrix} A & -B_uR^{-1}B^{T}_u \\ -Q & -A^T \end{bmatrix} \begin{bmatrix} x(t) \\ p(t) \end{bmatrix} 
\end{equation}
with initial conditions
\begin{equation*}
    \begin{split}
	& x(0) = x_0 \\
	& p(t_f) = 0
    \end{split}
\end{equation*}
that is, the costate also goes to $0$.

This means that the solution is the steady state solution of the equation. The problem is that the Hamiltonian equations are not stable. They are, in fact, symmetric to the imaginary axis.

To solve this, we diagonalize the system with \[
    \begin{bmatrix} \dot{z}_1(t) \\ \dot{z}_2(t) \end{bmatrix} = \begin{bmatrix} -\Lambda & 0 \\ 0 & \Lambda \end{bmatrix} \begin{bmatrix} z_1(t) \\ z_2(t) \end{bmatrix} 
\] in which $\Lambda$ are the right half-plane eigenvalues of the Hamiltonian. So $z_1$ are the stable eigenvalues while $z_2$ are the unstable. So the transformation is defined by \[
\begin{bmatrix} x(t) \\ p(t) \end{bmatrix} = \Psi \begin{bmatrix} z_1(t) \\ z_2(t) \end{bmatrix} 
\]. By analysing the solution and the steady-state of the solution, we reach at \[
K = R^{-1}B_u^{T}\Psi_{21}(\Psi_{11})^{-1}
\] where \[
P = \Psi_{21}(\Psi_{11})^{-1}
\].

So to define the steady-state solution for the LQR controller:
\begin{enumerate}
    \item Find eigenvalues and eigenvectors of the Hamiltonian equation
    \item Select the eigenvectors associated with the $n_x$ eigenvalues in the left half-plane
    \item Form the two required matrices from the first $n_x$ elements of these $n_x$ eigenvectors and the second $n_x$ elements of these $n_x$ eigenvectors respectively (extract the sub-matrices)
    \item Compute the steady-state Riccati solution
    \item Compute the steady-state feedback gain matrix
\end{enumerate}

Note it can be shown that the closed loop poles of the steady-state LQR are the left half-poles of the Hamiltonian equation.

\lecture{X}{19.06.2020}{Optimal Control}

\section*{Review}

Review of what's been seen so far.

Our system is
\[
    \dot{x} = Ax + Bu
\] \[
y = Cx + Du
\] 

The critical values (equivalent to roots of the transfer function) are the eigenvalues of the characteristic equation, that is \[
    det(\lambda I-A) = 0
\] .

Stability means that \[
    Re(\lambda_i)<0 \forall i
\] .

Control design.

Pole placement approach: if we have order $n$, define $n$ eigenvalues for the closed loop $P_1, \ldots, P_n$. Steps:
\begin{enumerate}
    \item Controlability check \[
    W_c = \left[ B | AB | \ldots | A^{n-1}B \right] 
\] $rank(W_c) = n \implies $ controlability
    \item Desired polynomial \[
	    P_d = \prod_{j=1}^{n} (\lambda-P_j) 
    \] . If the system is controllable we can achieve this polynomial as the characteristic function, that is, we define $B k$ such that \[
    P_{closed loop} = det(\lambda I - A + B k)
    \] \[
    = P_d
    \] .
\end{enumerate}

If we can't measure our state variables, we can design an \textbf{observer}. For that we need to check the observability of the system, that is, given \[
    W_o = \begin{bmatrix} C \\ CA \\ \ldots \\ CA^{n-1}\end{bmatrix} 
\] we need \[
rank(W_o) = n
\] . If that is verified, we determine the poles we want for the observer as \[
P_o = \prod_{i=1}^{n} (\lambda - P_i) 
\] and so we define L such that \[
P_{ob} = det(\lambda I - A + LC)
\] \[
P_{ob} = P_o
\] .

This way, our observer is defined as \[
    \hat{x} = A \hat{x} + Bu + L(y - \hat{y})
\] where \[
\hat{y} = C \hat{x} + Du
\] with $D=0$ for simplicity. In this scenario, our state feedback gain $k$ is applied to $\hat{x}$.


\section*{Optimal Control (LQR)}

Optimal control is the task of designing a controlling by looking beyond performance, so optimizing also the \emph{costs} associated.

The idea is that we want to define the control input  $\bm{u}(t)$ in an interval $t_0 \le t\le t_f$ such that the state of the system goes from an initial value to a desired final value minimizing a cost function. So we formulate this as an optimization problem with the initial and final states as the restrictions (more restrictions may exist). Note that the values that $\bm{x}(t)$ assumes during the time interval define a \emph{trajectory}.

Regarding the solutions to this problem:

\begin{enumerate}
    \item Certain combinations of cost functions and dynamic system models have analytic solutions;
    \item In a wide class of problems, we can find control laws;
    \item The speed of modern computers can perform the calculations in real time, which allows us to recompute the trajectories.
\end{enumerate}

\subsection*{The Optimal Control Problem}

The general optimization problem can be seen as the minimization of the cost function \[
J=\phi(\bm{x}(t_f),t_f) + \int_{t_o}^{t_f} \L(\bm{x}(t),\bm{u}(t),t)dt 
\] where $\phi$ is the \emph{final state penalty} and the integral of $\L$ is the cost associated with the states  trajectory. Note that $\phi$ depends on both the states on the final time and the final time $t_f$ itself, so we can formulate both the minimization of the final state position ($\bm{x}(t_f)$) and the minimization of the final time.

Also, note how the equation is limited to $t_f$, that is, we do not look after this moment, which means that the classical idea of stability (BIBO) does not make sense, as we are not looking to what happens after a certain point and, therefore, both the input and output are always bounded.

Back to the problem formulation, we also add a constraint \[
    \bm{c}(\bm{x}) = 0
\] which implies that the solution has to remain on the surface defined by $\bm{c}$.

We know that the states are dictated by the dynamics of the (linear) system. So the following must be satisfied \[
    \dot{\bm{x}} = A\bm{x} + B\bm{u}
\], that is, it is a constraint to the optimization if we formulate it as \[
    \dot{\bm{x}} - A\bm{x} - B\bm{u} = 0
\].

Therefore, our optimization problem can be stated as \[
    \min_{\bm{u}, t_f} J(\bm{x},\bm{u},t_f,t_0)
\] \[
\textrm{s.t. } \bm{c}(\bm{x}) = 0
\]. Of course this is subject to the problem and more constraints (other than that of the system model) can be added or the current can be loosened. \emph{E.g.,} the final state can be seen as a constraint.

So, then, to solve our optimization problem, we must find a solution to the derivatives of $J$ in the directions along the surface. That is, the gradients \[
    \frac{\partial J(\bm{x}^*)}{\partial x} + p \frac{\partial \bm{c}(\bm{x}^*)}{\partial x} = 0
\] for a solution $\bm{x}^*$. Which means we can now formulate a new unconstrained problem \[
    J_a(\bm{x}, p) = J(\bm{x}) + pc(\bm{x})
\] in which $p$ is the coefficient that defines the linear combination.

\subsection*{Optimization}

\begin{description}
    \item[Open-loop Parametric Optimization] when $\bm{u}(t)$ is assumed to be a known function defined by parameters $\bm{k}$, therefore we can write $\bm{u}(\bm{k},t)$.
\end{description}

In this case, the optimization problem can be seen as an optimization over $\bm{k}$. Therefore, we need to find \emph{local minima} of $J$, that is, we must find $\bm{k}$ such that \[
    \frac{\partial J}{\partial \bm{k}}=\bm{0}
\] and \[
\frac{\partial^{2}J}{\partial \bm{k}^{2}}>\bm{0}
\]. Note that this equations can be seen as a system of equations over the unknown variables $\bm{k}$, that is, in matrix form.

\begin{description}
    \item[Closed-loop Parametric Optimization] when  $\bm{u}(t)$ is a function over the dynamic state, that is, $\bm{u}(t) = \bm{\psi}[\bm{k},\bm{x}(t),t]$ where $\bm{\psi}[.]$ is the \emph{control law}.
\end{description}
